1
Efficient Visual Transformer by Learnable Token
Merging
Yancheng Wang and Yingzhen Yang
Abstract—Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating
transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for
computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging
(LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many
popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining
or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers,
including MobileViT, EfficientViT, ViT-S/16, and Swin-T, with LTM-Transformer blocks, leading to LTM-Transformer networks with different
backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound
for the IB loss is derived. The architecture of mask module in our LTM blocks which generate the token merging mask is designed to
reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders
compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers. The
code of the LTM-Transformer is available at https://github.com/Statistical-Deep-Learning/LTM.
Index Terms—Visual Transformers, Learnable Token Merging, Information Bottleneck, Variational Upper Bound, Compact Transformer
Networks
✦
1
INTRODUCTION
B
UILDING upon the success of Transformer in natu-
ral language processing [1],visual transformers have
demonstrated remarkable performance across a wide range
of tasks [2], [3], [4], [5], [6], [7]. However, the achievements
of visual transformers are accompanied with heavy compu-
tational costs [3], [8], making their deployment impractical
under resource-limited scenarios. The aforementioned lim-
itations have spurred recent research endeavors aimed at
developing efficient visual transformers. In this paper, we
study the problem of accelerating visual transformers by
token merging.
Token merging is an effective method for reducing the
FLOPs and improving the inference speed of visual trans-
formers [9], [10], [11], [12], [13], [14]. However, most existing
token merging methods [12], [13], [14], [15] largely sacrifice
the prediction accuracy of the original transformer networks
for reduced computation costs [14], [16]. These methods [12],
[14] generally focus on identifying and merging similar
tokens by averaging their features. However, such merging
strategies, which are based solely on feature similarity, can
potentially diminish the informative features in the tokens
that are critical to the prediction tasks. Therefore, it remains
an interesting and important question that if we can perform
token merging while preserving a compelling performance
of the visual transformers after token merging. To this
end, we propose a novel transformer block, Transformer
with Learnable Token Merging, or LTM-Transformer, which
learns how to merge tokens while exhibiting a compelling
generalization capability of the transformer with merged
tokens.
•
Yancheng Wang and Yingzhen Yang are with School of Computing and
Augmented Intelligence, Arizona State University, Tempe, AZ, 85281.
E-mail: ywan1053@asu.edu, yingzhen.yang@asu.edu
Motivation. Due to the fact that the FLOPs of a visual
transformer largely depend on the number of tokens in all
the transformer blocks, the FLOPs of a visual transformer can
be significantly reduced by reducing the number of tokens
in all the transformer blocks. Our goal is to merge the
output tokens of all the transformer blocks into less tokens
without largely sacrificing the prediction accuracy of the
original visual transformer. However, directly merging the
output tokens, even by carefully designed methods [12], [13],
[14], would adversely affect the performance of the model. In
this paper, we propose to maintain a compelling prediction
accuracy of a visual transformer with token merging by an
informative token merging process. In our LTM-Transformer
block, the original attention output tokens of a transformer
block are merged into less target tokens, and every target
token is an informative weighted average of the original
output tokens. All the target tokens, or merged tokens are
the final attention output tokens for the LTM-Transformer
block, which are fed to an MLP to produce the output of the
LTM-Transformer block as illustrated by Figure 1.
Such a token merging process in LTM-Transformer is
primarily inspired by the well-known presence of consider-
able redundancy in the original output tokens of transformer
blocks [14], [15]. As different tokens have varying importance
in modeling the visual features at a particular transformer
block, it is natural to compute an informative aggregation
of the original attention output tokens as the final (target)
attention output tokens so that more informative and more
important tokens contribute more to the merged tokens with
a larger weight in the weighted average in the aggregation
process.
Such an idea of informative token merging can also be
viewed from the perspective of Information Bottleneck (IB).
Let Z be the original attention output tokens, which are
arXiv:2407.15219v1  [cs.CV]  21 Jul 2024
2
merged into the merged tokens denoted by ˜X, and let
Y be the ground truth training labels for a classification
task. ˜X has less tokens than Z. The principle of IB is to
maximize the mutual information between ˜X and Y while
minimizing the mutual information between ˜X and X.
That is, IB encourages the network to learn the merged
tokens more correlated with the class labels while reducing
their correlation with the input. Extensive empirical and
theoretical works have evidenced that models respecting
the IB principle enjoy compelling generalization. With the
informative token merging process in LTM-Transformer, the
merged tokens ˜X are the informative aggregation of the
original attention output tokens Z, so ˜X are less correlated
with the training images and in this manner the IB principle
is better adhered. This is reflected in Table 5 in Section 4.4,
where a model for ablation study with token merging, ToMe,
enjoys less IB loss than the vanilla transformer, MobileViT-
S. This observation indicates that the IB principle is better
respected by the token merging process in ToMe. In order
to further decrease the IB loss, we propose an Information
Bottleneck (IB) inspired token merging process, where a LTM-
Transformer block generates an informative token merging
task which reduces the IB loss for visual transformers.
For example, our model termed “LTM-MobileViT-S” in
Table 5 is the visual transformer with the IB loss reduced
by replacing all the transformer blocks in MobileViT-S with
LTM-Transformer blocks so that more informative merged
tokens are generated by the proposed informative token
merging process. While ToMe hurts the prediction accuracy
compared to the vanilla model, our LTM-Transformer enjoys
even higher top-1 accuracy than the vanilla MobileViT-S,
and we have the same observations for MobileViT-XS and
EfficientViT.
1.1
Contributions
The contributions of this paper are presented as follows.
First, we present a novel and compact transformer block
termed Transformer with Learnable Token Merging, or
LTM-Transformer. Our LTM-Transformer block generates
an informative token merging mask which reduces the IB
loss. The LTM-Transformer blocks can be used to replace all
the transformer blocks in many popular visual transformers,
rendering compact visual transformers with competitive
performance. The effectiveness of LTM-Transformer is evi-
denced by replacing all the transformer blocks in popular
visual transformers which are already compact, including
MobileViT [17], EfficientViT [7], ViT-S/16 [3], and Swin-T [4],
with LTM-Transformer blocks, for image classification, object
detection and instance segmentation tasks.
Second, we propose an informative token merging pro-
cess for visual transformers which can reduces the IB loss.
As a first step, we derive a novel and separable variational
upper bound for the IB loss associated with token merging,
which is I( ˜X(G))−I( ˜X(G), Y ) where I(·, ·) denotes mutual
information and G is the token merging mask in LTM-
Transformer. We then view a transformer with multiple LTM-
Transformer blocks as an iterative process for reduction of
the IB loss by gradient descent, and every LTM-Transformer
block performs one-step gradient descent on the variational
upper bound for the IB loss. Inspired by this understanding,
the token merging mask at the current layer is generated
from the token merging mask at the previous layer and
the input tokens at the current layer by a learnable mask
module, following the formula of gradient descent as in (3)
in Section 3.2. As a result, such informative token merging
process generates in a network with LTM-Transformer blocks
enjoys reduced IB loss, which is evidenced in our ablation
study in Section 4.4. Due to the separability of the variational
upper bound for the IB loss, a neural network with LTM-
Transformer blocks can be trained in an end-to-end manner
with standard SGD. We remark that as shown in Table 5, a
baseline token merging method, ToMe, can already reduce
the IB loss. By replacing all the transformer blocks with our
LTM-Transformer blocks, the network with LTM-Transformer
enjoys smaller IB loss, higher classification accuracy and
less FLOPs. Importantly, extensive experiment results on
various computer vision tasks demonstrate the compelling
performance of LTM-Transformer networks compared to the
competing baselines.
It is worthwhile to mention that our LTM-Transformers
are trained from scratch, while most existing token merging
methods [12], [13], [14] perform token merging on a trained
transformer without additional training [14] or with a
light training process [12], [13]. The training time of LTM-
Transformers with comparison to competing token merging
methods is presented in Table 6 of Section 5.1.
This paper is organized as follows. The related works
in efficient visual transformers and compression of visual
transformers by pruning or token merging are discussed
in Section 2. The formulation of LTM-Transformer is de-
tailed in Section 3. The effectiveness of LTM-Transformer
is demonstrated in Section 4 for image classification, object
detection and instance segmentation tasks, by replacing all
the transformer blocks of various popular visual transform-
ers, including MobileViT [17], EfficientViT [7], ViT-S/16 [3],
and Swin-T [4], with LTM-Transformer blocks. Our ablation
studies are presented in Section 5 and Section 4.4, and the
proofs of theoretical results are in Section 6. We conclude
the paper in Section 7. Throughout this paper we use [n] to
denote the natural numbers between 1 and n inclusively.
2
RELATED WORKS
2.1
Efficient Visual Transformers
Visual transformer models have recently achieved superior
performance on a variety of computer vision applications [4],
[5], [6], [18], [19], [20]. However, visual transformers often
encounter high computational demands due to the quadratic
complexity of the point-wise attention and numerous Multi-
Layer Perceptron (MLP) layers. To mitigate the challenges
of high computational costs, various strategies have been
developed [2], [5], primarily aimed at refining the net-
work architectures and incorporating sparse mechanisms
for efficient computation. These include the integration of
convolutions into transformer networks [7], [17], [21], the
use of knowledge distillation for training more efficient
transformers [22], [23], [24], and compressing existing visual
transformers with methods such as pruning [25], [26], [27].
Techniques for compressing visual transformers generally
fall into three categories: (1) Channel Pruning, which targets
the elimination of superfluous heads and channels within
3
ViT blocks [25], [28], [29]; (2) Block Pruning, which involves
removing redundant transformer blocks [26], [30]; (3) Token
Pruning and Token Merging, which prune less important
tokens and merge similar tokens in the input of transformer
blocks [14], [15], [27], [31].
In this paper, we focus on learning to merge tokens
guided by the information bottleneck theory of deep learning
and primarily review existing works on Token Pruning
and Merging [12], [13], [14], [15], [31]. DynamicViT [15]
observes that the prediction in visual transformers is only
based on a subset of the most informative tokens and
proposes a hierarchical token sparsification framework to
prune redundant tokens. ToMe [14] proposes a graph-based
matching algorithm that combines similar tokens in each
visual transformer block of a pre-trained visual transformer.
LTMP [13] learns threshold masking modules that dynami-
cally determine which tokens to merge and prune in a unified
framework similar to DynamicViT. ToFu [12] also combines
token pruning and token merging. Instead of average merg-
ing similar tokens, ToFu proposes a conventional average
merging module to improve the quality of merged tokens.
2.2
Related Works about Information Bottleneck
[32] provides the first in-depth analysis of conventional
information bottleneck (IB) theories and deep learning to
establish the connection between the nonlinearity of neural
networks and the compression phase of training. Building
on the theory of IB, [33] proposes a probabilistic attention
module reducing mutual information between the input
and the masked representation while increasing mutual
information between the masked representation and the
task label. Further exploring the mechanics of IB in deep
learning, [34] finds that self-attention mechanisms can be
interpreted as iterative steps in optimizing the IB objective,
which explains the advantages of self-attention in learning
robust representation. Distinct from most existing methods
that implicitly incorporate the IB principle, our work adopts
a direct and innovative approach. We aim to optimize a
novel and separable variational upper bound of the IB loss
with a learnable token merging method. The proposed LTM-
Transformer lead to compelling performance on many popu-
lar visual transformer architecture with lower computation
cost, benefiting from the learnable token merging mechanism
guided by the IB principle.
3
FORMULATION
In this section, we first illustrate how to perform token
merging using a token merging mask. We then describe how
to generate the token merging mask from a learnable mask
module in a LTM-Transformer block, as well as the training
algorithm of a neural network with LTM-Transformer blocks.
We derive a novel and separable variational upper bound for
the IB loss, and the token merging masks are generated to
reduce such variational upper bound for the IB loss.
3.1
Token Merging by Learnable Token Merging Masks
Given the input feature tokens X ∈RN×D where N is the
number of tokens and D is the token dimension, the LTM-
Transformer block first applies the self-attention module on
the input feature tokens by Z = ATTN(X), where ATTN(·)
is the regular QKV self-attention operation [35]. As illustrated
in Figure 1, every LTM-Transformer block has an MLP as a
learnable mask module which generates the token merging
mask G(ℓ) where ℓis the index of the current layer or block.
The LTM-Transformer block merges the N tokens of Z into P
tokens with P < N by multiplying Z with the token merging
mask G(ℓ) ∈RN×P . We set P = ⌈r × N⌉, where r ∈(0, 1)
is termed the compression ratio for LTM, and a smaller r
renders less merged tokens after token merging. The token
merging mask G(ℓ) of the ℓ-th transformer block is generated
by the token merging mask G(ℓ−1) of the previous layer
and the feature tokens Z, which is motivated by reducing
the IB loss and detailed in Section 3.2. The token merging
mask G(1) for the first block is generated from the input
feature tokens of the first layers by G(1) = Softmax(f(X/τ)),
where f(·) represents an MLP layer followed by a Sigmoid
function. τ ∈(0, 1) is a temperature parameter that aims
to sparsify the merging mask. After obtaining the merging
mask G(ℓ), the features tokens of Z are merged into P tokens
by ˜X(G(ℓ)) =

Z⊤G(ℓ)⊤
∈RP ×D, which is then passed to
the following MLP layers in the transformer block.
In addition to merging tokens in regular transformer
blocks such as ViT [35] and Swin [4], the LTM-Transformer
block can also be applied to efficient transformer blocks
widely applied in efficient visual transformer architectures
such as MobileViT [17] and EfficientViT [7]. Regular trans-
former blocks obtain the output by sequentially applying
the attention operation and MLP on the input feature tokens.
However, efficient transformer blocks usually contain resid-
ual connections following the design of residual connections
in Convolutional Neural Networks (CNNs). That is, these
blocks maintain the same shapes for the input X and the
self-attention output Z and concatenate them to produce
the output features of the current transformer block. As a
result, we cannot only merge the tokens of Z. Instead, our
LTM-Transformer block merges the tokens of both X and Z
so that the number of merged tokens for X and Z have is the
same. To this end, we apply the same token merging mask
G(ℓ) to merge both X and Z. As a result, the compressed X
and Z are of the same shape after the token merging process
and they can still be concatenated, which is illustrated in
Figure 1(b). In addition, transformer blocks in the efficient
visual transformers are usually accompanied with convo-
lution operations so that they need to maintain the feature
tokens in a three-dimensional format X ∈RH×W ×D as
illustrated in Figure 1(b). To apply our token merging method
on efficient transformer blocks, we set the number of merged
tokens after token merging as P = H′ × W ′, where r is the
compression ratio, and H′ = ⌈H × √r⌉, W ′ = ⌈W × √r⌉.
Therefore, the merged tokens can still be reshaped into three-
dimensional features for later convolution operations. The
analysis about the inference computation cost, or the FLOPs,
of the LTM transformer block for token merging in both
regular transformers and efficient transformers as illustrated
in Figure 1 is detailed below.
Computation Cost Analysis of LTM-Transformer for To-
ken Marging. We hereby analyze the additional inference
computation cost, or the FLOPs, of the LTM transformer
block for token merging in both regular transformers and
4
Attention
MLP
Merging Mask
Merged Tokens
Input Tokens
Mask Module
. . . 
Input
Output
. . . 
(a) LTM-Transformer block for regular transformers, such as ViT and Swin.
Unfold
Attention
MLP
Conv
Concat
Input 
Unfold
Fold
Fold
Conv
Output
Merged Tokens
Input Tokens
. . . 
Merging Mask
Mask Module
. . . 
Matrix 
Multiplication
(b) LTM-Transformer block for efficient transformers, such as MobileViT and EfficientViT.
Fig. 1: Overall framework of Learnable Token Merging (LTM)-Transformer block for regular transformer blocks such as ViT
and Swin (a), and efficient transformer blocks such as MobileViT and EfficientViT (b).
efficient transformers as illustrated in Figure 1. Let D be the
dimension of input tokens and N be the number of tokens.
The FLOPs of the token merging in an LTM transformer block
in regular visual transformers is 6CDP +3C+ND2+NDP,
where 6CDP + 3C + ND2 is the FLOPs for calculating the
merging mask and NDP is the cost for applying the merging
mask on the input tokens. In the LTM transformer block of
efficient visual transformers, the additional FLOPs of the
token merging is 6CDP + 3C + ND2 + 2NDP, since the
merging mask will be applied to both the input tokens and
the merged tokens.
3.2
Generating Token Merging Mask by Reducing Varia-
tional Upper Bound for the IB Loss
We describe how to generate the token merging mask in a
LTM-Transformer block in this subsection, and the generation
of the token merging mask is inspired by reduction of the IB
loss. We first introduce the setup where the IB loss can be
specified.
Given the training data {Xi, yi}n
i=1 where Xi is the i-the
input training feature and yi is the corresponding class label.
Let Zi be the the self-attention output tokens of the Xi, and
˜Xi(G) = (ZiG)⊤is the merged tokens with G being the
token merging mask. We first specify how to compute the IB
loss, IB(G) = I( ˜X(G), X) −I( ˜X(G), Y ) which depends on
G and other network parameters, X is a random variable
representing the input feature which takes values in {Xi}n
i=1,
˜X(G) is a random variable representing the merged tokens
which takes values in
n
˜Xi(G)
on
i=1. Y is a random variable
representing the class label which takes values in {yi}n
i=1.
After performing K-means clustering on
n
˜Xi(G)
on
i=1 and
{Xi}n
i=1, we have the clusters
n
˜Ca
oC
a=1 and {Cb}C
b=1 for the
merged tokens and the input features respectively, where C
is the number of classes. We also abbreviate ˜X(G) as ˜X for
simplicity of the notations. Then we define the probability
that ˜X belongs to cluster ˜Ca as Pr
h
˜X ∈a
i
= 1
n
nP
i=1
ϕ( ˜Xi, a)
with ϕ( ˜Xi, a)
=
exp

−∥˜
Xi−˜Ca∥
2
2

PA
a=1 exp

−∥˜
Xi−˜Ca∥
2
2
. Similarly, we
define the probability that Xi belongs to cluster Cb as
Pr [X ∈b] =
1
n
nP
i=1
ϕ(Xi, b). Moreover, we have the joint
probabilities Pr
h
˜X ∈a, X ∈b
i
=
1
n
nP
i=1
ϕ( ˜Xi, a)ϕ(Xi, b)
and Pr
h
˜X ∈a, Y = y
i
= 1
n
nP
i=1
ϕ( ˜Xi, a)1I{yi=y} where 1I{}
is an indicator function. As a result, we can compute
the mutual information I( ˜X(G), X) and I( ˜X(G), Y ) by
I( ˜X(G), X) =
C
X
a=1
C
X
b=1
Pr
h
˜X(G) ∈a, X ∈b
i
log
Pr
h
˜X(G) ∈a, X ∈b
i
Pr
h
˜X(G) ∈a
i
Pr [X ∈b]
,
I( ˜X(G), Y ) =
C
X
a=1
C
X
y=1
Pr
h
˜X(G) ∈a, Y = y
i
log
Pr
h
˜X ∈a, Y = y
i
Pr
h
˜X(G) ∈a
i
Pr [Y = y]
,
and then compute the IB loss IB(G). As explained in
5
our motivation, we aim to perform token merging while
can reduce the IB loss. However, directly optimizing the
IB loss in the standard SGD training is difficult as the
IB loss is not separable. Given a variational distribution
Q( ˜X ∈a|Y = y) for y, a ∈[C] computed by Eq. (8) in
Section 6.3, the following theorem gives a variational upper
bound, IBB(G)), for the IB loss IB(G)). IBB(G)) is separable
and thus compatible with SGD training with minibatches.
Theorem 3.1.
IB(G) ≤IBB(G) −C0,
(1)
where C0 is a constant only depending on the input training
features {Xi}n
i=1, and
IBB(G) := 1
n
n
X
i=1
C
X
a=1
C
X
b=1
ϕ( ˜
Xi(G), a)ϕ(Xi, b) log ϕ(Xi, b)
−1
n
n
X
i=1
C
X
a=1
C
X
y=1
ϕ( ˜
Xi(G), a)1I{yi=y} log Q( ˜
X ∈a|Y = y).
Proposition 3.2. Suppose
˜Xi(G) =
 Z⊤
i G
⊤∈RP ×D
with Zi
∈
RN×D being the self-attention output tokens
for the i-th training feature and G
∈
RN×P
is the
token merging mask where N
is the number of tokens,
D is the token dimension, P
is the number of merged
tokens after token merging, and
˜Xi(G) denotes the merged
tokens. At step ℓof gradient descent on IBB(G), we have
G(ℓ) = G(ℓ−1) −η∇GIBB(G(ℓ−1))
= G(ℓ−1) −2η
n
n
X
i=1
C
X
a=1
Zi
S(l−1)
ia
(γ(l−1)
i
)2

γ(l−1)
i
Ca −ζ(ℓ−1)
i

ψi,a, ℓ≥2,
(2)
where S(ℓ)
ia
:= exp

−
 ˜Xi(G(ℓ)) −˜Ca

2
2

for i ∈[n] and
a ∈[C], γ(ℓ)
i
:=
C
P
a=1
S(ℓ)
ia , ζ(ℓ)
i
:=
C
P
a=1
S(ℓ)
ia ˜Ca for i ∈[n], ψi,a :=
C
P
b=1
ϕ(Xi, b) log ϕ(Xi, b) −
C
P
y=1
1I{yi=y} log Q( ˜X ∈a|Y = y).
The proofs of Theorem 3.1 and Proposition 3.2 are
deferred to Section 6. Inspired by Proposition 3.2, we can
understand a transformer with token merging and multiple
transformer blocks as an iterative process which reduces
IBB(G) by gradient descent, where the ℓ-th transformer block
performs one-step gradient descent on IBB(G) according to
(2). The mask module of at the ℓ-th LTM-Transformer block
generates the token merging mask G(ℓ) from G(ℓ−1), the
token merging mask of the previous block, through (2). To
improve the flexibility of the token merging mask, an MLP
is applied on Zi. Moreover, as IBB and ∇GIBB are separable,
(2) can be performed on a minibatch Bj ⊆{1, . . . , n}, which
is compatible with minibatch-based training with SGD for a
transformer network with LTM-Transformer blocks. That
is, the mask module of the ℓ-th LTM-Transformer block
generates G(ℓ) by
G(ℓ) = G(ℓ−1)
−2η
n
X
i∈Bj
C
X
a=1
MLP(Zi) S(l−1)
ia
(γ(l−1)
i
)2

γ(l−1)
i
Ca −ζ(l−1)
i

ψi,a,
(3)
where MLP(·) consists of two linear layers of the same
hidden dimension with a ReLU activation in the middle, and
each LTM-Transforer block has its own MLP.
Algorithm 1 Training Algorithm of LTM-Transformers
1: Initialize the weights of the network by W = W(0) through
random initialization, set ttrain which is the number of
training epochs.
2: for t ←1 to ttrain do
3:
if t < twarm then
4:
Perform gradient descent by a standard step of SGD
without applying token merging in LTM transformer
blocks.
5:
else
6:
Update ϕ( ˜
Xi, a) for all the clusters a ∈[C] and i ∈[n].
7:
for j ←1 to J do
8:
Forward
step:
generate
n
G(ℓ)o
for
all
the
LTM-Transformer
blocks
by
(3)
using
the
minibatch Bj, the updated
n
ϕ( ˜
Xi, a)
o
i∈Bj,a∈[C],
n
Q(t−1)( ˜
X ∈a|Y = y)
o
a∈[C],y∈[C],
and
n
˜C(t−1)
a
oC
a=1, as well as the output of the network
9:
Backward step: update the MLP layers of the mask
modules in all the LTM-Transformer blocks as well
as all the other weights in the neural network by a
standard step of SGD on the cross-entropy loss
10:
end for
11:
Compute Q(t)( ˜
X ∈a|Y = y) by Eq. (8) in Section 6.3,
perform K-means clustering on { ˜
Xi}n
i=1 and update the
clusters
n
˜C(t)
a
oC
a=1.
12:
end if
13: end for
14: return The trained weights W of the network
Algorithm 1 describes the training process of a neural
network with LTM-Transformer blocks using the standard
cross-entropy loss for a classification problem. It is remarked
that all the MLP layers of the mask modules in all the LTM-
Transformer blocks, along with other network parameters,
are updated with standard SGD. In order to generate the
token merging masks for all the LTM-Transformer blocks
before a new epoch starts, we update the variational dis-
tribution Q(t) and the clusters
n
˜C(t)
a
oC
a=1 at the end of the
previous epoch.
4
EXPERIMENTAL RESULTS
In this section, LTM-Transformers are assessed for the image
classification task on the ImageNet-1k dataset. The results in
Section 4.1 indicate that LTM outperforms existing state-
of-the-art networks while maintaining a more compact
architecture. In addition, LTM is compared with existing
methods on token merging and shows better performance
with lower computation costs. Furthermore, in Sections 4.2
and 4.3, we demonstrate that the use of LTM-MobileViT
and LTM-EfficientViT as feature extraction backbones leads
to superior mAP and reduced FLOPs compared to the
baseline models for the tasks of object detection and semantic
segmentation. In Section 4.4, we perform ablation studies
on the effects of LTM-Transformer in reducing the IB loss
and the IB bound at different layers of a LTM-Transformer
network.
6
0.4
0.6
0.8
1.0
1.2
1.4
FLOPs (G)
74
76
78
80
Top-1 Accuracy (%)
MobileViT
EfficientViT
ToMe-MobileViT
ToMe-EfficientViT
ToFu-MobileViT
ToFu-EfficientViT
LTMP-MobileViT
LTMP-EfficientViT
LTM-MobileViT
LTM-EfficientViT
Fig. 2: Top-1 accuracy vs FLOPs (G) on ImageNet-1k valida-
tion set.
4.1
Image Classification
Implementation details. ImageNet [36] classification, we
employ MobileViT-S [17], MobileViT-XS [17], EfficientViT-
B1 [7], ViT-S [35], and Swin-T [4] as backbone architectures.
We substitute the conventional transformer blocks in these
backbones with LTM-Transformer blocks. During the training
of LTM-Transformers, we utilize the AdamW optimizer with
β1 = 0.9 and β2 = 0.999. The training process spans 300
epochs, starting with a warm-up phase during which token
merging is not applied in all the LTM-transformer blocks.
After the warm-up stage, we enable token merging in all
the LTM-transformer blocks. twarm is fixed to 100 in all the
experiments. All the experiments are conducted on four
NVIDIA A100 GPUs with a total batch size of 1024 images.
Following prior works [4], our training incorporates popular
data augmentation methods such as RandAugment, Mixup,
Cutmix, and random erasing. We set the weight decay at
0.01. The learning rate initially increases from 0.0002 to 0.002
over the first 10 epochs and is subsequently reduced back
to 0.0002 following a cosine decay schedule. We set η in (2)
to 1 in all the experiments. In addition, we apply a softmax
operation on the token merging mask at each layer to ensure
the aggregation weights for each merged tokens sum to 1.
Results. As shown in Table 1, models integrated with
LTM-Transformer blocks show reduced FLOPs and enhanced
accuracy compared to their original visual transformer
counterparts, at a cost of very slight increase of model size
due to the extra parameters in the mask modules of the
LTM-Transformer blocks. For instance, LTM-MobileViT-S
not only reduces its FLOPs from 1.4G to 1.17G but also
improves accuracy by 1.3% over the original MobileViT-S,
with a slight increase of the model size by 0.3M. Similarly,
LTM-MobileViT-XS achieves a 1.0% accuracy increase while
lowering its FLOPs from 0.7G to 0.52G compared to the
original model, with a slight increase of model size by 0.2M.
To further demonstrate the efficiency of the LTM-Transformer,
we compare it against current state-of-the-art weight prun-
ing methods for efficient visual transformers, including
S2ViTE [38], SPViT [39], and SAViT [40] on EfficientViT-B1
(r224). To apply S2ViTE, SPViT, and SAViT on EfficientViT-
B1 (r224), we first run their pruning process following the
standard implementation in their papers [38], [39], [40] on the
Model
# Params FLOPs Top-1
MobileViT-XS
2.3 M
0.7 G
74.8
ToMe-MobileViT-XS [14]
2.3 M 0.54 G
72.7
ToFu-MobileViT-XS [12]
2.3 M 0.54 G
73.3
LTMP-MobileViT-XS [13]
2.3 M 0.56 G
73.9
LTM-MobileViT-XS (Ours)
2.5 M 0.52 G
75.8
Mobile-Former
9.4 M
0.2 G
76.7
EfficientFormer [37]
12.3 M
1.3 G
79.2
MobileViT-S
5.6 M
1.4 G
78.4
ToMe-MobileViT-S [14]
5.6 M 1.22 G
76.7
ToFu-MobileViT-S [12]
5.6 M 1.22 G
77.2
LTMP-MobileViT-S [13]
5.6 M 1.26 G
77.5
LTM-MobileViT-S (Ours)
5.9 M 1.17 G
79.7
EfficientViT-B1 [r224] [7]
9.1 M 0.52 G
79.4
S2ViTE-EfficientViT-B1 [r224] [38]
8.2 M 0.47 G
79.0
SPViT-EfficientViT-B1 [r224] [39]
9.2 M 0.49 G
79.3
SAViT-EfficientViT-B1 [r224] [40]
8.4 M 0.47 G
79.2
ToMe-EfficientViT-B1 [r224] [14]
9.1 M 0.47 G
78.8
ToFu-EfficientViT-B1 [r224] [12]
9.1 M 0.47 G
79.0
LTMP-EfficientViT-B1 [r224] [13]
9.1 M 0.50 G
79.2
LTM-EfficientViT-B1 [r224] (Ours)
9.5 M 0.44 G
80.2
EfficientViT-B1 [r288] [7]
9.1 M 0.86 G
80.4
ToMe-EfficientViT-B1 [r288] [14]
9.1 M 0.73 G
79.7
ToFu-EfficientViT-B1 [r288] [12]
9.1 M 0.73 G
79.8
LTMP-EfficientViT-B1 [r288] [13]
9.1 M 0.76 G
80.0
LTM-EfficientViT-B1 [r288] (Ours)
9.5 M 0.70 G
81.0
ViT-S/16 [3]
22.1 M
4.3 G
81.2
LTM-ViT-S/16 (Ours)
23.0 M
3.7 G
81.8
Swin-T [4]
29.0 M
4.5 G
81.3
LTM-Swin-T (Ours)
30.5 M
3.8 G
81.8
TABLE 1: Comparisons with baseline methods on ImageNet-
1k validation set.
ImageNet training data. After obtaining the pruned networks,
we fine-tune them using the same setting as in [7]. As LTM-
Transofmer focuses on token merging, we compare our LTM-
Transformer models with existing token merging methods,
including ToMe [14], ToFu [12], and LTMP [13]. All the token
merging methods, ToMe [14], ToFu [12], LTMP [13], and our
LTM-Transformer, are applied on the same visual transformer
backbone. It is observed from Table 1 that with even lower
FLOPs, LTM-Transformer models consistently outperform
the competing token merging methods.
In addition, we apply LTM-Transformer to ViT-B with
two different FLOPs budgets and compare LTM-Transformer
with ViT-B models accelerated by ToMe, ToFu, and LTMP
in Table 2. The inference time of all the models are also
evaluated on the validation set of ImageNet-1k and reported
in milliseconds (ms) per batch for an evaluation batch size
of 128 on one Nvidia A100 GPU. It is observed that LTM-
Transformer models outperform the competing token merg-
ing methods with even lower FLOPs and faster inference
speed. Remarkably, LTM-ViT-B with 12.85G FLOPs has even
higher top-1 accuracy than the vanilla ViT-B, evidencing the
effectiveness of the informative token merging process in
LTM-Transformer blocks.
4.2
Object Detection
Implementation details. We incorporate ImageNet pre-
trained models, that are LTM-MobileViT-XS, LTM-MobileViT-
7
Methods
# Params. FLOPs Inference Time (ms/batch) Top-1
ViT-B
86.5 M
17.58 G
37.2
83.74
ViT-B (ToMe [14])
86.5 M
8.78 G
25.4
78.88
ViT-B (ToFu [12])
86.5 M
8.78 G
26.0
80.70
ViT-B (LTMP [13])
86.5 M
8.84 G
27.1
81.21
LTM-ViT-B
90.0 M
8.30 G
25.2
82.23
ViT-B (ToMe [14])
86.5 M
13.12 G
31.0
82.86
ViT-B (ToFu [12])
86.5 M
13.12 G
31.5
83.22
ViT-B (LTMP [13])
86.5 M
13.46 G
32.7
83.29
LTM-ViT-B
90.4 M
12.85 G
30.7
83.87
TABLE 2: Performance comparison with token pruning
methods on ImageNet.
S, and LTM-EfficientViT, with the single-shot object detec-
tion backbone, SSDLite [41], to evaluate on the MS-COCO
dataset [42], which comprises 117k training images and
5k validation images. We fine-tune all pre-trained LTM-
Transformers within the object detection framework at
a standard input resolution of 320 × 320. These models
undergo a training period of 200 epochs using the AdamW
optimizer, adhering to the training protocols established
in [17]. Employing a cosine learning rate scheduler, the
initial learning rate of 0.0009 is gradually reduced to 1.6e−6.
For the object localization, we utilize a smooth ℓ1 loss,
and for classification, cross-entropy losses are applied. The
evaluation of performance on the validation set is conducted
using the mAP metric with an IoU range from 0.50 to 0.95 in
increments of 0.05.
Feature backbone
# Params. FLOPs mAP
MobileNetv3 [43]
4.9 M
1.4 G
22.0
MobileNetv2 [41]
4.3 M
1.6 G
22.1
MobileNetv1 [44]
5.1 M
2.6 G
22.2
MixNet [45]
4.5 M
2.2 G
22.3
MNASNet [46]
4.9 M
1.7 G
23.0
YoloV5-N (640×640) [47]
1.9 M
4.5 G
28.0
Vidt [48]
7.0 M
6.7 G
28.7
MobileViT-XS
2.7 M
1.7 G
24.8
LTM-MobileViT-XS(Ours)
2.9 M
1.5 G
25.4
MobileViT-S
5.7 M
2.4 G
27.7
LTM-MobileViT-S(Ours)
6.0 M
2.1 G
28.4
EfficientViT
9.9 M
1.5 G
28.4
LTM-EfficientViT(Ours)
10.3 M
1.4 G
28.9
TABLE 3: Object detection performance with SSDLite.
Results. We adopt a comparative study of our LTM
Transformers against other lightweight feature backbones
within the SSDLite object detection framework. The results,
as detailed in Table 3, illustrate significant improvements in
object detection performance when the feature backbone is
upgraded to include LTM-Transformer blocks. For example,
substituting MobileViT-S with LTM-MobileViT-S enhances
the mAP by 0.7% while concurrently reducing FLOPs by
0.3G. Additionally, SSDLite equipped with LTM-EfficientViT
achieves a substantial performance increase of 6.9% while
maintaining the same FLOPs as MobileNetV3.
4.3
Instance Segmentation
In this section, we assess the efficacy of LTM when applied to
instance segmentation tasks using the COCO dataset [42]. We
utilize Mask R-CNN [49] equipped with a Feature Pyramid
Network (FPN) as the segmentation head, built on the LTM-
EfficientViT-B1 feature backbone. For comparative analysis,
we include EfficientViT-B1 [7] and EViT [21] as baseline
models. Both our models and the baselines are trained on
the training split of the COCO dataset and evaluated on the
validation split, adhering to the protocols established by [50].
The training duration is set to 12 epochs, consistent with
the 1× schedule described in [50]. The AdamW optimizer
is employed for training following the practices of [21]. We
initiate the learning rate at 0.001, which is then gradually
reduced following a cosine learning rate schedule. Perfor-
mance metrics reported include the mean bounding box
Average Precision (mAPb) and mean mask Average Precision
(mAPm), along with bounding box Average Precision (APb)
and mask Average Precision (APm) at IoU thresholds of
0.5 and 0.75. The findings, detailed in Table 4, demonstrate
that LTM-EfficientViT-B1 consistently enhances segmentation
performance across various thresholds.
Methods
mAPbox
APb
50
APb
75
mAPm
APm
50
APm
75
EViT [21]
32.8
54.4
34.5
31.0
51.2
32.2
EfficientViT-B1 [7]
33.5
55.4
34.8
31.9
52.3
32.7
LTM-EfficientViT-B1
34.3
56.1
35.2
32.8
52.8
33.1
TABLE 4: Instance Segmentation Results on COCO.
4.4
Ablation Study
Study on the effects of LTM in reducing IB loss. We study
the effectiveness of LTM-Transformer in reducing the IB loss
and the variational upper bound of IB loss, which is the IB
bound, across three visual transformers, including MobileViT-
S, MobileViT-XS, and EfficientViT (r224). We compare the
performance of the visual transformers with the baseline
token merging method, ToME [14], and the corresponding
LTM-Tranformer models with all the tansformer blocks
replaced with the LTM-Transformer blocks. The results are
shown in Table 5. The results indicate that although ToMe
reduces the IB loss and the IB bound, thereby adhering to the
IB principle which aims to enhance the correlation of features
with class labels while reducing their correlation with the
input, LTM can further decrease the IB loss and IB bound. In
particular, our LTM-Transformer models improve the vanilla
visual transformers and the ToMe models by a large margin
in terms of both IB loss and top-1 accuracy.
Model
# Params
FLOPs
Top-1
IB Bound
IB Loss
MobileViT-S
5.6 M
1.40 G
78.4
0.05782
-0.00432
ToMe-MobileViT-S
5.6 M
1.22 G
76.7
0.04542
-0.00913
LTM-MobileViT-S
5.9 M
1.17 G
79.7
0.02791
-0.01773
MobileViT-XS
2.3 M
0.71 G
74.8
0.05539
-0.00419
ToMe-MobileViT-XS
2.3 M
0.54 G
72.7
0.04583
-0.00647
LTM-MobileViT-XS
2.5 M
0.52 G
75.8
0.03082
-0.01618
EfficientViT (r224)
9.1 M
0.52 G
79.4
0.06014
-0.00451
ToMe-EfficientViT (r224)
9.1 M
0.47 G
78.8
0.04642
-0.00732
LTM-EfficientViT (r224)
9.5 M
0.44 G
80.2
0.02886
-0.01539
TABLE 5: Ablation Study on the effects of LTM in reducing
IB loss.
Study on the IB loss and IB bound at different layers.
To study how the IB loss IB(G), and the variational upper
bound for the IB loss, IBB(G), decrease with respect to layer
index ℓof a LTM-Transformer network, IB(G) and IBB(G)
8
1
2
3
4
5
6
7
8
9
Layer Index
−0.025
0.000
0.025
0.050
0.075
0.100
0.125
IB Bound
MobiltViT-S (100 Epochs)
LTM-MobiltViT-S (100 Epochs)
MobiltViT-S (300 Epochs)
LTM-MobiltViT-S (300 Epochs)
(a) IB bound (IBB(G)) comparison between MobileViT-S and
LTM-MobileViT-S.
1
2
3
4
5
6
7
8
9
Layer Index
−0.04
−0.03
−0.02
−0.01
0.00
0.01
IB Loss
MobiltViT-S (100 Epochs)
LTM-MobiltViT-S (100 Epochs)
MobiltViT-S (300 Epochs)
LTM-MobiltViT-S (300 Epochs)
(b) IB loss (IB(G)) comparison between MobileViT-S and
LTM-MobileViT-S.
Fig. 3: IB bound and IB loss comparison between MobileViT-S and LTM-MobileViT-S at different transformer layers.
Methods
# Params
FLOPs
Training Time (Hours)
Top-1
MobileViT-XS
2.3 M
0.70 G
73.5
75.8
ToMe-MobileViT-XS
2.3 M
0.54 G
73.5
72.7
ToFu-MobileViT-XS
2.3 M
0.54 G
73.5
73.3
LTMP-MobileViT-XS
2.3 M
0.56 G
73.8
73.9
LTM-MobileViT-XS
2.5 M
0.52 G
91.0
76.8
MobileViT-S
5.6 M
1.40 G
89.5
78.4
ToMe-MobileViT-S
5.6 M
1.22 G
89.5
76.7
ToFu-MobileViT-S
5.6 M
1.22 G
89.5
77.2
LTMP-MobileViT-S
5.6 M
1.17 G
90.0
77.5
LTM-MobileViT-S
5.9 M
1.22 G
105.0
79.7
EfficientViT-B1 [r224]
9.1 M
0.52 G
73.0
79.4
ToMe-EfficientViT-B1 [r224]
9.1 M
0.47 G
73.0
78.8
ToFu-EfficientViT-B1 [r224]
9.1 M
0.47 G
73.0
79.0
LTMP-EfficientViT-B1 [r224]
9.1 M
0.50 G
73.3
79.2
LTM-EfficientViT-B1 [r224]
9.5 M
0.44 G
91.0
80.2
EfficientViT-B1 [r288]
9.1 M
0.86 G
95.5
80.4
ToMe-EfficientViT-B1 [r288]
9.1 M
0.73 G
95.5
79.7
ToFu-EfficientViT-B1 [r288]
9.1 M
0.73 G
95.5
89.8
LTMP-EfficientViT-B1 [r288]
9.1 M
0.76 G
95.9
80.0
LTM-EfficientViT-B1 [r288]
9.5 M
0.70 G
110.5
81.0
TABLE 6: Training time (minutes/epoch) comparisons between LTM-Transformers and their baseline models.
for both MobileViT-S and LTM-MobileViT-S across different
transformer layers are illustrated in Figure 3. Both models
contain 9 transformer layers. It is observed from Figure 3
that both IB(G) and IBB(G) decrease in deeper layers with
larger layer indices of MobileViT-S and LTM-MobileViT-S.
This observation suggests that features in deeper layers
correlate more closely with the class labels and less with
the input features, adhering to the IB principle. Moreover,
LTM-MobileViT-S reduces both IB(G) and IBB(G) to lower
levels in deeper layers compared to MobileViT-S. These
observations evidence that the mask module in the LTM-
Transformer block which generates the informative token
merging task by (3) can effectively reduce both IB(G) and
IBB(G), better adhering to the IB principle than the vanilla
MobileViT-S. At the early stage of the training after 100
epochs, the IB bound and the IB loss of LTM-MobileViT-S
are similar to those of the MobileViT-S. After training for 300
epochs, the IB bound and the IB loss of LTM-MobileViT-S are
much smaller than those of the MobileViT-S.
5
MORE ABLATION STUDIES
We conduct more ablation studies in this section, including
the training time comparison, the training loss and test loss
of LTM-Transformers, and the visualization results illus-
trating the effectiveness of a LTM-Transformer in selecting
informative tokens during the token merging process. We
compare the training time of LTM-Transformer models with
the competing baselines for token merging in Table 6 in
Section 5.1. Figure 4 in Section 5.2 illustrates the training
loss and the test loss during the training process of LTM-
Transformers and their baselines, highlighting that the test
loss of the LTM-Transformer networks exhibits a more rapid
decline compared to that of their baselines.
5.1
Training Time Evaluation
We evaluate the training cost of our LTM-Transformer models
and the baseline models on the training set of ImageNet-
1k. The training is performed on 4 NVIDIA A100 GPUs
with an effective batch size of 512 images. We report the
9
0
50
100
150
200
250
300
Epochs
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
Training Loss
MobileViT-XS
LTM-MobileViT-XS
(a) Training loss comparison between
MobileViT-XS and LTM-MobileViT-XS.
0
50
100
150
200
250
300
Epochs
0.9
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Training Loss
MobileViT-S
LTM-MobileViT-S
(b) Training loss comparison between
MobileViT-S and LTM-MobileViT-S.
0
50
100
150
200
250
300
Epochs
1.0
1.2
1.4
1.6
1.8
2.0
Training Loss
EfficientViT-B1
LTM-EfficientViT-B1
(c) Training loss comparison between
EfficientViT-B1 and LTM-EfficietViT-B1.
0
50
100
150
200
250
300
Epochs
1.0
1.2
1.4
1.6
1.8
Test Loss
MobileViT-XS
LTM-MobileViT-XS
(d)
Test
loss
comparison
between
MobileViT-XS and LTM-MobileViT-XS.
0
50
100
150
200
250
300
Epochs
1.0
1.2
1.4
1.6
1.8
Test Loss
MobileViT-S
LTM-MobileViT-S
(e)
Test
loss
comparison
between
MobileViT-S and LTM-MobileViT-S.
0
50
100
150
200
250
300
Epochs
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Test Loss
EfficientViT-B1
LTM-EfficientViT-B1
(f)
Test
loss
comparison
between
EfficientViT-B1 and LTM-EfficietViT-B1.
Fig. 4: Training loss and test loss comparison between LTM-Transformer networks and corresponding baseline models.
overall training time of 300 epochs. We also include the
training time of ToMe [14], ToFu [12], and LTMP [13] for
comparison. It is noted that ToMe, ToFu, and LTMP are
applied to pre-trained models. Therefore, the training time
for ToMe, ToFu, and LTMP includes the training time of
the baseline models. In contrast, our models are trained
from scratch. The training time of various models are shown
in Table 6. The training overhead of LTM-Transformers
mainly comes from the computation of
n
ϕ( ˜Xi, a)
o
i∈Bj,a∈[C],
n
Q(t−1)( ˜X ∈a|Y = y)
o
a∈[C],y∈[C], and
n
˜C(t−1)
a
oC
a=1 as de-
scribed in Algorithm 1. It is observed from the Table 6
that the training time of LTM models is comparable to
the training time of the competing token merging methods.
In addition, LTM largely resolves the issue of significant
prediction accuracy drops after token merging by ToMe,
ToFu, and LTMP.
5.2
Training Loss and Test Loss of LTM-Transfomers
In this section, we illustrate the training loss and the test
loss of LTM-MobileViT-XS, LTM-MobileViT-S, and LTM-
EfficientViT-B1. In comparison, we also illustrate the train-
ing loss and test loss of MobileViT-XS, MobileViT-S, and
EfficientViT-B1. All the models are trained for 300 epochs.
The plots are illustrated in Figure 4. It can be observed that
LTM-Transformer networks achieve lower training losses and
test losses at the end of the training, which demonstrates the
benefit of LTM-Transformers in improving the performance
of the visual transformers through the IB-inspired token
merging.
5.3
Visualization Results
To study the effectiveness of LTM-Transformer in select-
ing informative tokens during the token merging process,
we visualize the token merging masks in the first LTM-
Transformer block of LTM-MobileViT-S for selected images
from ImageNet in Figure 5. Each image is divided into
16 × 16 tokens. For each example, we select only the most
representative merged token that encapsulates the critical
features of the objects in the image, and the merged token is
a weighted average of several self-attention output tokens
with the aggregation weights in the token merging mask.
The input images are illustrated in the first row, and the
heatmaps that visualize the aggregation weights in the token
merging mask for the selected merged token are shown in
the second row. The class labels for each image are presented
at the bottom of each column. The results illustrate that the
mask module in the LTM-Transformer block usually assigns
higher aggregation weights to tokens covering the most
representative and distinctive parts of the objects, which
are often the most informative for classifying the images.
In the example of the dhole in the first column, the LTM-
Transformer block puts larger weights on the eyes and nose
of the dhole. In the example of the hartebeest in the second
column, the LTM-Transformer block puts larger weights
on the twisted horns of the hartebeest. In the example of
the racing car in the third column, the LTM-Transformer
block puts larger weights on the wheel of the car. In the
example of the Stethoscope in the fourth column, the LTM-
Transformer block puts larger weights on the diaphragm of
the stethoscope. These observations demonstrate that more
informative tokens contribute more to the merged tokens
10
1
0
Images
Merged Tokens
Label: Racing Car
Label: Dhole
Label: Hartebeest
Label: Stethoscope
Fig. 5: Visualization of merging weights in the first LTM-Transformer block in LTM-MobileViT-S.
with larger aggregation weights in the token merging process
of the LTM-Transformer block.
6
PROOFS
6.1
Proof of Proposition 3.2
Proof. We
first
compute
the
gradient
of
ϕ( ˜Xi(G), a′)
with
respect
to
˜Xi(G)
by
∇˜
Xi(G)ϕ( ˜Xi(G), a′) =
2Sia′
C
P
a=1
Sia

˜Xi(G) −˜Ca

−2Sia′

˜Xi(G) −˜Ca′

C
P
a=1
Sia
 C
P
a=1
Sia
2
.
Using the definitions of γi and ζi as γi :=
C
P
a=1
Sia and
ζi :=
C
P
a=1
Sia ˜Ca for i ∈[n], we have
∇˜
Xi(G)ϕ( ˜Xi(G), a′) = 2Sia′
γ2
i

γi ˜Ca′ −ζi

.
As
a
result,
the
gradient
of
IBB(G)
with
respect
to
G
is
computed
as
follows:
∇GIBB(G) = 1
n
n
X
i=1
C
X
a=1
C
X
b=1
Zi∇˜
Xiϕ( ˜
Xi, a)ϕ(Xi, b) log ϕ(Xi, b)
−1
n
n
X
i=1
C
X
a=1
C
X
y=1
Zi∇˜
Xiϕ( ˜
Xi, a)1I{yi=y} log Q( ˜
X ∈a|Y = y)
= 1
n
n
X
i=1
C
X
a=1
Zi∇˜
Xiϕ( ˜
Xi, a)ψi,a
= 2
n
n
X
i=1
C
X
a=1
Zi Sia
γ2
i

γi ˜Ca −ζi

ψi,a,
where
ψi,a
:=
C
P
b=1
ϕ(Xi, b) log ϕ(Xi, b)
−
C
P
y=1
1I{yi=y} log Q( ˜X ∈a|Y = y).
6.2
Proof of Theorem 3.1
We need the following two lemmas before the proof of
Theorem 3.1. It is noted that we abbreviate ˜X(G) and ˜Xi(G)
as ˜X and ˜Xi in the sequel.
Lemma 6.1.
I( ˜X, X) ≤1
n
n
X
i=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b) log ϕ(Xi, b)
−1
n2
n
X
i=1
n
X
j=1
C
X
b=1
ϕ(Xi, b) log ϕ(Xj, b)
(4)
Lemma 6.2.
I( ˜X, Y ) ≥1
n
C
X
a=1
C
X
y=1
n
X
i=1
ϕ( ˜Xi, a)1I{yi=y} log Q( ˜X ∈a|Y = y)
(5)
Proof of Theorem 3.1. We note that IB(W) = I( ˜X, X) −
I( ˜X, Y ).
Then
IB(W)
≤
IBB(W) −C0
follows
by
the upper bound for I( ˜X, X) in Lemma 6.1 and the
lower bound for I( ˜X, Y ) in Lemma 6.2. Here C0
=
1
n2
nP
i=1
nP
j=1
C
P
b=1
ϕ(Xi, b) log ϕ(Xj, b).
Proof of Lemma 6.1. By the log sum inequality, we have
I( ˜X, X)
=
C
X
a=1
C
X
b=1
Pr
h
˜X ∈a, X ∈b
i
log
Pr
h
˜X ∈a, X ∈b
i
Pr
h
˜X ∈a
i
Pr [X ∈b]
≤1
n2
n
X
i=1
n
X
j=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b)

log

ϕ( ˜Xi, a)ϕ(Xi, b)

−log

ϕ( ˜Xi, a)ϕ(Xj, b)

= 1
n2
n
X
i=1
n
X
j=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b) log ϕ(Xi, b)
−1
n2
n
X
i=1
n
X
j=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b) log ϕ(Xj, b)
= 1
n
n
X
i=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b) log ϕ(Xi, b)
11
−1
n2
n
X
i=1
n
X
j=1
C
X
a=1
C
X
b=1
ϕ( ˜Xi, a)ϕ(Xi, b) log ϕ(Xj, b).
(6)
Proof of Lemma 6.2. Let Q( ˜X|Y ) be a variational distribu-
tion. We have
I( ˜X, Y )
=
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log
Pr
h
˜X ∈a, Y = y
i
Pr
h
˜X ∈a
i
Pr [Y = y]
=
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log
Pr
h
˜X ∈a|Y = y
i
Q( ˜X ∈a|Y = y)
Pr
h
˜X ∈a
i
Q( ˜X ∈a|Y = y)
≥
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log
Pr
h
˜X ∈a|Y = y
i
Q( ˜X ∈a|Y = y)
+
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log Q( ˜X ∈a|Y = y)
Pr
h
˜X ∈a
i
= KL

P( ˜X|Y )
Q( ˜X|Y )

+
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log Q( ˜X ∈a|Y = y)
Pr
h
˜X ∈a
i
≥
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log Q( ˜X ∈a|Y = y)
Pr
h
˜X ∈a
i
=
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log Q( ˜X ∈a|Y = y) + H

P( ˜X)

≥
C
X
a=1
C
X
y=1
Pr
h
˜X ∈a, Y = y
i
log Q( ˜X ∈a|Y = y)
≥1
n
C
X
a=1
C
X
y=1
n
X
i=1
ϕ( ˜Xi, a)1I{yi=y} log Q( ˜X ∈a|Y = y).
(7)
6.3
Computation of Q(t)( ˜X|Y )
The variational distribution Q(t)( ˜X|Y ) can be computed by
Q(t)( ˜X ∈a|Y = y) = Pr
h
˜X ∈a|Y = y
i
=
nP
i=1
ϕ( ˜Xi, a)1I{yi=y}
nP
i=1
1I{yi=y}
.
(8)
7
CONCLUSION
In this paper, we propose a novel transformer block, Trans-
former with Learnable Token Merging, or LTM-Transformer.
LTM-Transformer blocks perform token merging so as to
render a transformer network with less FLOPs and faster
inference speed. A LTM-Transformer block generates an
informative token merging mask for token merging in
a learnable manner, which is inspired by the reduction
of the Information-Bottleneck loss. A network with LTM-
Transformer blocks can be trained with standard SGD, and it
enjoys a reduction of IB loss and reduced FLOPs while main-
taining a compelling prediction accuracy. We demonstrate the
effectiveness of LTM-Transformer by replacing all the trans-
former blocks in several popular visual transformers with
LTM-Transformer blocks. Extensive experiments on various
tasks demonstrate the effectiveness of LTM-Transformer.
REFERENCES
[1]
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
in Advances in neural information processing systems, 2017, pp. 5998–
6008.
[2]
L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z. Jiang, F. E. Tay, J. Feng,
and S. Yan, “Tokens-to-token vit: Training vision transformers from
scratch on imagenet,” in Proceedings of the IEEE/CVF international
conference on computer vision, 2021.
[3]
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in International
Conference on Learning Representations, 2021.
[4]
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted
windows,” arXiv preprint arXiv:2103.14030, 2021.
[5]
X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:
Deformable transformers for end-to-end object detection,” ICLR,
2021.
[6]
J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,
“Swinir: Image restoration using swin transformer,” ICCV, 2021.
[7]
H. Cai, C. Gan, and S. Han, “Efficientvit: Enhanced linear attention
for high-resolution low-computation visual recognition,” in Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision,
2023.
[8]
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J´egou, “Training data-efficient image transformers & distillation
through attention,” in International conference on machine learning.
PMLR, 2021.
[9]
S. Han, J. Pool, J. Tran, and W. Dally, “Learning both weights and
connections for efficient neural network,” in Advances in Neural
Information Processing Systems, 2015, pp. 1135–1143.
[10] Z. Zhou, H. Yao, X. Guo, and Y. Xu, “Rethinking the value of
network pruning,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2020, pp. 5867–5875.
[11] W. Sun, Y. Jiang, S. Zhang, and Y. Liu, “Cascade pruning: Towards
class imbalance in convolutional neural networks,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 9860–9869.
[12] M. Kim, S. Gao, Y.-C. Hsu, Y. Shen, and H. Jin, “Token fusion:
Bridging the gap between token pruning and token merging,” in
Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, 2024, pp. 1383–1392.
[13] M. Bonnaerens and J. Dambre, “Learned thresholds token merging
and pruning for vision transformers,” Transactions on Machine
Learning Research, 2023.
[14] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoff-
man, “Token merging: Your vit but faster,” ICLR, 2023.
[15] Y. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh, “Dynamicvit:
Efficient vision transformers with dynamic token sparsification,”
arXiv preprint arXiv:2106.02034, 2021.
[16] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, “Attention
augmented convolutional networks,” in Proceedings of the IEEE/CVF
international conference on computer vision, 2019, pp. 3286–3295.
[17] S. Mehta and M. Rastegari, “Mobilevit: light-weight, general-
purpose, and mobile-friendly vision transformer,” ICLR, 2022.
[18] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in ICLR, 2021.
[19] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S. Zagoruyko, “End-to-end object detection with transformers,” in
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part I 16.
Springer, 2020.
[20] Y. Wang, N. Xu, C. Chen, and Y. Yang, “Adaptive cross-layer
attention for image restoration,” arXiv preprint arXiv:2203.03619,
2022.
[21] X. Liu, H. Peng, N. Zheng, Y. Yang, H. Hu, and Y. Yuan, “Effi-
cientvit: Memory efficient vision transformer with cascaded group
attention,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 14 420–14 430.
[22] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. J´egou,
and M. Douze, “Levit: a vision transformer in convnet’s clothing
for faster inference,” in Proceedings of the IEEE/CVF international
conference on computer vision, 2021, pp. 12 259–12 269.
12
[23] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll´ar,
“Designing network design spaces,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2020.
[24] C. Gong, D. Wang, M. Li, X. Chen, Z. Yan, Y. Tian, qiang liu, and
V. Chandra, “NASVit: Neural architecture search for efficient vision
transformers with gradient conflict aware supernet training,” in
International Conference on Learning Representations, 2022.
[25] T. Chen, Y. Cheng, Z. Gan, L. Yuan, L. Zhang, and Z. Wang, “Chas-
ing sparsity in vision transformers: An end-to-end exploration,”
Advances in Neural Information Processing Systems, 2021.
[26] F. Yu, K. Huang, M. Wang, Y. Cheng, W. Chu, and L. Cui, “Width &
depth pruning for vision transformers,” in Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 36, no. 3, 2022, pp. 3143–3151.
[27] Z. Kong, P. Dong, X. Ma, X. Meng, W. Niu, M. Sun, X. Shen, G. Yuan,
B. Ren, H. Tang et al., “Spvit: Enabling faster vision transformers
via latency-aware soft token pruning,” in Computer Vision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XI.
Springer, 2022, pp. 620–640.
[28] A. Chavan, Z. Shen, Z. Liu, Z. Liu, K.-T. Cheng, and E. P.
Xing, “Vision transformer slimming: Multi-dimension searching
in continuous optimization space,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2022, pp.
4931–4941.
[29] C. Zheng, K. Zhang, Z. Yang, W. Tan, J. Xiao, Y. Ren, S. Pu et al.,
“Savit: Structure-aware vision transformer pruning via collaborative
optimization,” Advances in Neural Information Processing Systems,
vol. 35, pp. 9010–9023, 2022.
[30] S. Yu, T. Chen, J. Shen, H. Yuan, J. Tan, S. Yang, J. Liu, and Z. Wang,
“Unified visual transformer compression,” ICLR, 2022.
[31] Z. Wang, H. Luo, P. Wang, F. Ding, F. Wang, and H. Li, “Vtc-lfc:
Vision transformer compression with low-frequency components,”
Advances in Neural Information Processing Systems, vol. 35, 2022.
[32] A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky,
B. D. Tracey, and D. D. Cox, “On the information bottleneck
theory of deep learning,” Journal of Statistical Mechanics: Theory
and Experiment, vol. 2019, no. 12, p. 124020, 2019.
[33] Q. Lai, Y. Li, A. Zeng, M. Liu, H. Sun, and Q. Xu, “Information
bottleneck approach to spatial attention learning,” in Proceedings
of the Thirtieth International Joint Conference on Artificial Intelligence,
IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021,
Z. Zhou, Ed.
ijcai.org, 2021, pp. 779–785.
[34] D. Zhou, Z. Yu, E. Xie, C. Xiao, A. Anandkumar, J. Feng, and J. M.
Alvarez, “Understanding the robustness in vision transformers,”
in International Conference on Machine Learning.
PMLR, 2022, pp.
27 378–27 394.
[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” in 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021.
OpenReview.net, 2021.
[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet
large scale visual recognition challenge,” International journal of
computer vision, vol. 115, no. 3, pp. 211–252, 2015.
[37] Y. Li, G. Yuan, Y. Wen, J. Hu, G. Evangelidis, S. Tulyakov, Y. Wang,
and J. Ren, “Efficientformer: Vision transformers at mobilenet
speed,” Advances in Neural Information Processing Systems, vol. 35,
pp. 12 934–12 949, 2022.
[38] T. Chen, Y. Cheng, Z. Gan, L. Yuan, L. Zhang, and Z. Wang, “Chas-
ing sparsity in vision transformers: An end-to-end exploration,”
Advances in Neural Information Processing Systems, vol. 34, 2021.
[39] Z. Kong, P. Dong, X. Ma, X. Meng, W. Niu, M. Sun, X. Shen, G. Yuan,
B. Ren, H. Tang et al., “Spvit: Enabling faster vision transformers
via latency-aware soft token pruning,” in European Conference on
Computer Vision.
Springer, 2022, pp. 620–640.
[40] C. Zheng, K. Zhang, Z. Yang, W. Tan, J. Xiao, Y. Ren, S. Pu et al.,
“Savit: Structure-aware vision transformer pruning via collaborative
optimization,” Advances in Neural Information Processing Systems,
vol. 35, pp. 9010–9023, 2022.
[41] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceed-
ings of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510–4520.
[42] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision.
Springer, 2014.
[43] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,
W. Wang, Y. Zhu, R. Pang, V. Vasudevan et al., “Searching for
mobilenetv3,” in Proceedings of the IEEE/CVF International Conference
on Computer Vision, 2019, pp. 1314–1324.
[44] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient
convolutional neural networks for mobile vision applications,”
arXiv preprint arXiv:1704.04861, 2017.
[45] M. Tan and Q. V. Le, “Mixconv: Mixed depthwise convolutional
kernels,” in Proceedings of the British Machine Vision Conference
(BMVC), 2019.
[46] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
and Q. V. Le, “Mnasnet: Platform-aware neural architecture search
for mobile,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2019, pp. 2820–2828.
[47] J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2017, pp. 7263–7271.
[48] H. Song, D. Sun, S. Chun, V. Jampani, D. Han, B. Heo, W. Kim,
and M.-H. Yang, “Vidt: An efficient and effective fully transformer-
based object detector,” ICLR, 2022.
[49] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 2961–2969.
[50] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,
Z. Liu, J. Xu et al., “Mmdetection: Open mmlab detection toolbox
and benchmark,” arXiv preprint arXiv:1906.07155, 2019.
